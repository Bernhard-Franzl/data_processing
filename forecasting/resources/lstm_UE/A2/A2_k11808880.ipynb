{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ea3c26",
   "metadata": {},
   "source": [
    "# Assignment 2: Training the Fully Recurrent Network\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "\n",
    "## Exercise 1: Data generation\n",
    "\n",
    "There are two classes, both occurring with probability 0.5. There is one input unit. Only the first sequence element conveys relevant information about the class. Sequence elements at positions $t > 1$ stem from a Gaussian with mean zero and variance 0.2. The first sequence element is 1.0 (-1.0) for class 1 (2). Target at sequence end is 1.0 (0.0) for class 1 (2)\n",
    "\n",
    "Write a function `generate_data` that takes an integer `T` as argument which represents the sequence length. Seed the `numpy` random generator with the number `0xDEADBEEF`. Implement the [Python3 generator](https://docs.python.org/3/glossary.html#term-generator) pattern and produce data in the way described above. The input sequences should have the shape `(T, 1)` and the target values should have the shape `(1,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04244bb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T16:44:39.573530Z",
     "start_time": "2022-12-07T16:44:38.915316Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FullyRecurrentNetwork(object):\n",
    "    def __init__(self, D, I, K):\n",
    "        self.W = np.random.uniform(-0.01, 0.01, (I, D))\n",
    "        self.R = np.random.uniform(-0.01, 0.01, (I, I))\n",
    "        self.V = np.random.uniform(-0.01, 0.01, (K, I))\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # helper function for numerically stable loss\n",
    "        def f(z):\n",
    "            return np.log1p(np.exp(-np.absolute(z))) + np.maximum(0, z)\n",
    "        \n",
    "        # infer dims\n",
    "        T, D = x.shape\n",
    "        K, I = self.V.shape\n",
    "\n",
    "        # init result arrays\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.a = np.zeros((T, I))\n",
    "\n",
    "        # iterate forward in time \n",
    "        # trick: access model.a[-1] in first iteration\n",
    "        for t in range(T):\n",
    "            self.a[t] = np.tanh(self.W @ x[t] + self.R @ self.a[t-1])\n",
    "            \n",
    "        self.z = model.V @ self.a[t]\n",
    "        return y * f(-self.z) + (1-y) * f(self.z)\n",
    "\n",
    "T, D, I, K = 10, 3, 5, 1\n",
    "model = FullyRecurrentNetwork(D, I, K)\n",
    "model.forward(np.random.uniform(-1, 1, (T, D)), 1)\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(0xDEADBEEF)\n",
    "\n",
    "def generate_data(T):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    \n",
    "    # initialize random number generator\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # create array for sequence\n",
    "        x = np.zeros((T,1))\n",
    "        x[0] = rng.choice([1.0, -1.0], size=1, p=[0.5,0.5]) #1,-1\n",
    "        x[1:T] = rng.normal(loc=0.0, scale=np.sqrt(0.2), size=(T-1,1))\n",
    "\n",
    "        # create target values depending on class\n",
    "        if x[0] == 1.0:\n",
    "            y = np.array([1]) #1\n",
    "        else:\n",
    "            y = np.array([0]) #0\n",
    "\n",
    "        yield x,y\n",
    "        \n",
    "data = generate_data(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9826f26",
   "metadata": {},
   "source": [
    "## Exercise 2: Gradients for the network parameters\n",
    "Compute gradients of the total loss \n",
    "$$\n",
    "L = \\sum_{t=1}^T L(t), \\quad \\text{where} \\quad L(t) = L(z(t), y(t))\n",
    "$$\n",
    "w.r.t. the weights of the fully recurrent network. To this end, find the derivative of the loss w.r.t. the logits and hidden pre-activations first, i.e., \n",
    "$$\n",
    "\\psi^\\top(t) = \\frac{\\partial L}{\\partial z(t)} \\quad \\text{and} \\quad \\delta^\\top(t) = \\frac{\\partial L}{\\partial s(t)}.\n",
    "$$\n",
    "With the help of these intermediate results you should be able to compute the gradients w.r.t. the weights, i.e., $\\nabla_W L, \\nabla_R L, \\nabla_V L$. \n",
    "\n",
    "*Hint: Take a look at the computational graph from the previous assignment to see the functional dependencies.*\n",
    "\n",
    "*Remark: Although we only have one label at the end of the sequence, we consider the more general case of evaluating a loss at every time step in this exercise (many-to-many mapping).*\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400655c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\psi^\\top(t) &= \\frac{\\partial L}{\\partial z^{T}(t)}\n",
    "= \\sum^{T}_{t=1} \\frac{\\partial L(t)}{\\partial z^{T}(t)}\n",
    "=\\sum^{T}_{t=1} \\frac{\\partial L(\\sigma(z(t)),y(t))}{\\partial z^{T}(t)} \n",
    "= \\sum^{T}_{t=1} (\\hat y(t) - y(t))^{T} \\\\ \n",
    "& \\Rightarrow \\text{ To derive the derivative above I refer to assignment 1, where we already showed that} \n",
    "\\\\\\\\\n",
    "\\delta^\\top(t) &= \\frac{\\partial L}{\\partial s^{T}(t)}\n",
    "= \\sum^{T}_{t=1} \\frac{\\partial L(t)}{\\partial s^{T}(t)} \n",
    "= \\sum^{T}_{t=1} (\\frac{\\partial L(t+1)}{\\partial z(t+1)} \\frac{\\partial z(t+1)}{\\partial a(t+1)} \\frac{\\partial a(t+1)}{\\partial s(t+1)} \\frac{\\partial s(t+1)}{\\partial a(t)} + \\frac{\\partial L(t)}{\\partial z(t)} \\frac{\\partial z(t)}{\\partial a(t)}) \\frac{\\partial a(t)}{\\partial s(t)}\\\\\n",
    "&= \\sum^{T}_{t=1} (\\delta^\\top(t+1) \\frac{\\partial s(t+1)}{\\partial a(t)} + \\frac{\\partial L(t)}{\\partial z(t)} \\frac{\\partial z(t)}{\\partial a(t)}) \\frac{\\partial a(t)}{\\partial s(t)}\\\\\n",
    "&= \\sum^{T}_{t=1} (\\delta^\\top(t+1) \\cdot R^{T} +  \\psi^\\top(t) \\cdot V) \\cdot \\text{diag}(1-\\tanh^{2}s(t))\n",
    "\\\\\\\\\n",
    "\\frac{\\partial L}{\\partial v_{ki}} &= \\sum^{T}_{t=1} \\frac{\\partial L(t)}{\\partial v_{ki}} \n",
    "= \\sum^{T}_{t=1} \\frac{\\partial L(t)}{\\partial z(t)} \\frac{\\partial z(t)}{\\partial v_{ki}}\n",
    "= \\sum^{T}_{t=1} \\frac{\\partial L(t)}{\\partial z_{k}(t)} \\frac{\\partial z_{k}(t)}{\\partial v_{ki}}\n",
    "= \\sum^{T}_{t=1} \\psi_{k}(t)\\tanh(s_{i}(t))\\\\\n",
    "& \\Rightarrow \\text{ Can be brought into outer product representation:} \\\\\n",
    "& \\frac{\\partial L}{\\partial V} = \\sum^{T}_{t=1} \\psi(t)\\tanh(s(t))^{T}\n",
    "\\\\\\\\\n",
    "\\frac{\\partial L}{\\partial w_{id}} &= \\sum^{T}_{t=1} \\frac{\\partial L(t)}{\\partial w_{id}} \n",
    "= \\sum^{T}_{t=1} \\frac{\\partial L(t)}{\\partial s(t)}\\frac{\\partial s(t)}{\\partial w_{id}}\n",
    "= \\sum^{T}_{t=1} \\frac{\\partial L(t)}{\\partial s_{i}(t)}\\frac{\\partial s_{i}(t)}{\\partial w_{id}}\n",
    "= \\sum^{T}_{t=1} \\delta_{i}(t)x_{d}(t)\\\\\n",
    "& \\Rightarrow \\text{ Can be brought into outer product representation:} \\\\\n",
    "& \\frac{\\partial L}{\\partial W} = \\sum^{T}_{t=1} \\delta(t)x(t)^{T}\n",
    "\\\\\\\\\n",
    "\\frac{\\partial L}{\\partial r_{ij}} &= \\sum^{T}_{t=1} \\frac{\\partial L(t)}{\\partial r_{ij}} \n",
    "= \\sum^{T}_{t=1} \\frac{\\partial L(t)}{\\partial s(t)}\\frac{\\partial s(t)}{\\partial r_{ij}}\n",
    "= \\sum^{T}_{t=1} \\frac{\\partial L(t)}{\\partial s_{i}(t)}\\frac{\\partial s_{i}(t)}{\\partial r_{ij}}\n",
    "= \\sum^{T}_{t=1} \\delta_{i}(t)a_{j}(t-1)\\\\\n",
    "& \\Rightarrow \\text{ Can be brought into outer product representation:} \\\\\n",
    "& \\frac{\\partial L}{\\partial R} = \\sum^{T}_{t=1} \\delta(t)a(t-1)^{T}\n",
    "\\\\\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21968f2c",
   "metadata": {},
   "source": [
    "### Exercise 3: The backward pass\n",
    "Write a function `backward` that takes a model `self` as argument. The function should compute the gradients of the loss with respect to all model parameters and store them to `self.dW`, `self.dR`, `self.dV`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e3d13c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T16:44:39.580474Z",
     "start_time": "2022-12-07T16:44:39.574553Z"
    }
   },
   "outputs": [],
   "source": [
    "def backward(self):\n",
    "    \n",
    "    ########## YOUR SOLUTION HERE ##########   \n",
    "    \n",
    "    T, D = self.x.shape\n",
    "    K, I = self.V.shape\n",
    "    \n",
    "    self.dW = np.zeros(self.W.shape)\n",
    "    self.dR = np.zeros(self.R.shape)\n",
    "    self.dV = np.zeros(self.V.shape)\n",
    "    \n",
    "    delta = np.zeros((T+1, I, 1))\n",
    "    psi = np.zeros((T, K, 1))\n",
    "    psi[-1] = np.reshape((sigmoid(self.z) - self.y),(-1,1))\n",
    "    \n",
    "    for t in range(T-1,-1,-1):\n",
    "        delta[t] = ((delta[t+1].T @ self.R + psi[t].T @ self.V) @ np.diag(1- self.a[t]**2)).T\n",
    "        self.dV += psi[t] @ np.reshape(self.a[t],(1,-1))\n",
    "        self.dW += delta[t] @ np.reshape(self.x[t], (1,-1))\n",
    "        self.dR += delta[t] @ np.reshape(self.a[t-1], (1,-1))\n",
    "        \n",
    "FullyRecurrentNetwork.backward = backward\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58181c6",
   "metadata": {},
   "source": [
    "## Exercise 4: Gradient checking\n",
    "Write a function `grad_check` that takes a model `self`, a float `eps` and another float `thresh` as arguments and computes the numerical gradients of the model parameters according to the approximation\n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x + \\varepsilon) - f(x - \\varepsilon)}{2 \\varepsilon}.\n",
    "$$\n",
    "If any of the analytical gradients are farther than `thresh` away from the numerical gradients the function should throw an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "227e8631",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T16:44:39.603337Z",
     "start_time": "2022-12-07T16:44:39.581965Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grad_check(self, eps, thresh):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    \n",
    "    T, D = self.x.shape\n",
    "    K, I = self.V.shape\n",
    "    \n",
    "    def num_grad(parameter,eps):\n",
    "\n",
    "        rows, cols = parameter.shape\n",
    "        num_grad_matrix = np.zeros((rows,cols))\n",
    "        \n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                # plus\n",
    "                parameter[row][col] += eps\n",
    "                L_plus = self.forward(self.x, self.y)\n",
    "\n",
    "                # minus\n",
    "                parameter[row][col] -= 2*eps\n",
    "                L_minus = self.forward(self.x, self.y)\n",
    "                parameter[row][col] += eps\n",
    "\n",
    "                # num grad\n",
    "                num_grad_matrix[row][col] = (L_plus - L_minus)/(2*eps)\n",
    "                \n",
    "        return num_grad_matrix\n",
    "    \n",
    "    num_dW = num_grad(self.W, eps)\n",
    "    num_dV = num_grad(self.V, eps)\n",
    "    num_dR = num_grad(self.R, eps)\n",
    "    \n",
    "    dW_check = np.all(np.abs(num_dW - self.dW) <= thresh)\n",
    "    dV_check = np.all(np.abs(num_dV - self.dV) <= thresh) \n",
    "    dR_check = np.all(np.abs(num_dR - self.dR) <= thresh)\n",
    "    \n",
    "    return dW_check and dV_check and dR_check\n",
    "\n",
    "FullyRecurrentNetwork.grad_check = grad_check\n",
    "model.grad_check(1e-7, 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5cdc05",
   "metadata": {},
   "source": [
    "## Exercise 5: Parameter update\n",
    "\n",
    "Write a function `update` that takes a model `self` and a float argument `eta`, which represents the learning rate. The method should implement the gradient descent update rule $\\theta \\gets \\theta - \\eta \\nabla_{\\theta}L$ for all model parameters $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e93c02c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T16:44:39.608138Z",
     "start_time": "2022-12-07T16:44:39.604840Z"
    }
   },
   "outputs": [],
   "source": [
    "def update(self, eta):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "    self.W = self.W - eta * self.dW\n",
    "    self.V = self.V - eta * self.dV\n",
    "    self.R = self.R - eta * self.dR\n",
    "\n",
    "FullyRecurrentNetwork.update = update\n",
    "model.update(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19801bb8",
   "metadata": {},
   "source": [
    "## Exercise 6: Network training\n",
    "\n",
    "Train the fully recurrent network with 32 hidden units. Start with input sequences of length one and tune the learning rate and the number of update steps. Then increase the sequence length by one and tune the hyperparameters again. What is the maximal sequence length for which the fully recurrent network can achieve a performance that is better than random? Visualize your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d9781ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T16:44:43.425208Z",
     "start_time": "2022-12-07T16:44:39.609264Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########## YOUR SOLUTION HERE ##########    \n",
    "D, I, K = 1, 32, 1\n",
    "\n",
    "results = dict()\n",
    "\n",
    "T_max = 30\n",
    "\n",
    "for T in range(1,T_max+1):\n",
    "    \n",
    "    model = FullyRecurrentNetwork(D, I, K) \n",
    "    \n",
    "    updates = 200\n",
    "    lr = 1\n",
    "    \n",
    "    loss = []\n",
    "    \n",
    "    i = 1\n",
    "    for x,y in generate_data(T):\n",
    "\n",
    "        cur_loss = model.forward(x, y)\n",
    "        loss.append(cur_loss)\n",
    "        \n",
    "        model.backward()\n",
    "\n",
    "        model.update(lr)\n",
    "\n",
    "        if i == updates:\n",
    "            break\n",
    "        i += 1\n",
    "        \n",
    "    results[T] = loss\n",
    "    \n",
    "\n",
    "# get the sequence lengths \n",
    "sequence_lengths = list(results.keys())\n",
    "# calculate mean loss\n",
    "mean_losses = 1 - np.mean(np.array(list(results.values())), axis=1) \n",
    "\n",
    "# make nice plot\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(sequence_lengths, mean_losses)\n",
    "plt.xticks(sequence_lengths)\n",
    "plt.hlines(0.5, 0,31, color=\"red\") # indicates where loss is worse than random\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b52057",
   "metadata": {},
   "source": [
    "## Exercise 7: The Vanishing Gradient Problem\n",
    "\n",
    "Analyze why the network is incapable of learning long-term dependencies. Show that $\\|\\frac{\\partial a(T)}{\\partial a(1)}\\|_2 \\leq \\|R\\|_2^{T-1}$ , where $\\|\\cdot\\|_2$ is the spectral norm, and discuss how that affects the propagation of error signals through the time dimension of the network. \n",
    "\n",
    "*Hint: Use the fact that the spectral norm is submultiplicative for square matrices, i.e. $\\|AB\\|_2 \\leq \\|A\\|_2\\|B\\|_2$ if $A$ and $B$ are both square.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158130a5",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5ff95b",
   "metadata": {},
   "source": [
    "We need to show that the network looses information when backpropagating it through time. This is done by taking a closer look at $\\delta$ in the gradients. We see that inside the $\\delta$ we always have $\\frac{\\partial a(t+1)}{\\partial a(t)}$ and in fact we alwas have in every $\\delta (t)$ the $\\delta (t+1) inside$. This means in a very nested way the $\\delta (1)$ is connected to the $\\delta (T)$. Therefore, the derivative $\\frac{\\partial a(T)}{\\partial a(1)}$ is hidden in there and of great importance.\n",
    "\n",
    "The Proof:\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    " \\frac{\\partial a(T)}{\\partial a(1)} &= \\frac{\\partial a(T)}{\\partial a(T-(T-1))} \\\\\n",
    " &= R^{T-1} \\cdot (\\Pi^{T-2}_{i=0} diag(f'(s(T-i)))) &\\text{, with f'(.) being derivative of activation function} \\\\\n",
    " & \\Rightarrow \\text{both terms are square matrices}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "Now since we have calulated the result we are interested what the spectral norm of this gradient is. Since it would be nice to know if we can find some bounds for the underlying matrix multiplication.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\| \\frac{\\partial a(T)}{\\partial a(1)} \\|_2 &= \\| R^{T-1} \\cdot (\\Pi^{T-2}_{i=0} diag(f'(s(T-i)))) \\|_2\\\\\n",
    "& \\leq \\| R^{T-1}\\|_2 \\cdot \\|(\\Pi^{T-2}_{i=0} diag(f'(s(T-i)))) \\|_2 \\\\\n",
    "& \\leq \\| R^{T-1}\\|_2 \\cdot (\\Pi^{T-2}_{i=0} \\|diag(f'(s(T-i))) \\|_2 )\\\\\n",
    "(\\text{since the derivative of tanh is always}\\leq 1) &\\leq \\| R^{T-1}\\|_2 \\cdot k & \\text{, with k being some scalar smaller equal than one} \\\\\n",
    "&\\leq \\| R^{T-1}\\|_2 \\\\\n",
    "&\\leq \\| R\\|_2^{T-1}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "So we can see that the upper bound for the derivative means is $\\| R\\|_2^{T-1}$. The problem is that the spectral norm is taken to the power of $T-1$ , which can lead to three scenarios: <br>\n",
    "* $\\| R\\|_2 \\leq 1 \\Rightarrow$ derivative will decrease even more $\\Rightarrow$ Vanishing Gradient\n",
    "* $\\| R\\|_2 = 1 \\Rightarrow$ derivative will stay stable \n",
    "* $\\| R\\|_2 \\geq 1 \\Rightarrow$ derivative will increase even more $\\Rightarrow$ Exploding Gradient\n",
    "\n",
    "Since in practice $\\| R\\|_2 = 1$ will never really happen if not enforced, this will lead to unstable gradients.\n",
    "These unstable gradients are the reason why the network is incapable of learning long-term dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d3054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
